# LLM Output Processor

> Ingest and search semantically indexed memory using OpenAI embeddings and Qdrant vector database.

---

## ğŸ“¦ Project Structure

```
llm_output_processor/
â”œâ”€â”€ app/                 # Core FastAPI application
â”‚   â”œâ”€â”€ main.py          # App entry point
â”‚   â”œâ”€â”€ router.py        # API routes (/ingest, /search, /health)
â”‚   â”œâ”€â”€ qdrant_client.py # Qdrant upsert/search logic
â”‚   â”œâ”€â”€ auth.py          # Token verification
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py    # Loguru logger config
â”‚       â””â”€â”€ openai_client.py # OpenAI embeddings client
â”œâ”€â”€ docker-compose.yml   # Docker Compose orchestration
â”œâ”€â”€ Dockerfile           # App container definition
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ logs/                # Processor logs (ensure this exists)
â”œâ”€â”€ qdrant_data/         # Qdrant persistent storage
â”œâ”€â”€ data/                # General data storage (optional)
â””â”€â”€ README.md            # Project overview
```

---

## ğŸš€ Setup

### 1. Prerequisites
- Docker + Docker Compose
- OpenAI API key

### 2. Clone & Configure
Create a `.env` file (for local dev):
```
OPENAI_API_KEY=your_openai_key
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

Alternatively, set these in **docker-compose.yml** under `environment`.

### 3. Build & Run
```bash
docker compose up --build -d
```

Qdrant UI: [http://localhost:6333/dashboard](http://localhost:6333/dashboard)  
API Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## âœ… API Endpoints

- `POST /ingest`: Ingest a note for semantic indexing.
- `GET /search?q=query`: Semantic search.
- `GET /health`: Health check.

### Example Ingest
```bash
curl -X POST http://localhost:8000/ingest \
  -H "Authorization: Bearer <your_token>" \
  -H "Content-Type: application/json" \
  -d '{ "id": "check-1", "type": "note", "data": {"note": "test embedding"} }'
```

---

## ğŸ§© Components

- **Qdrant**: High-performance vector database for storage & search.
- **OpenAI Embeddings**: `text-embedding-3-small` model for vectorization.
- **FastAPI**: API layer for ingestion, search, and health check.
- **Loguru**: Structured logging to `logs/processor.log`.

---

## ğŸ› ï¸ Development

Rebuild after code changes:
```bash
docker compose build
docker compose up -d
```

Logs:
```bash
tail -f logs/processor.log
```

---

## âœ… Roadmap
- Dynamic collection versioning
- Batch ingestion
- Health endpoints with dimension checks
- Voice command integration

---

## ğŸ”’ Security
Ensure API tokens are rotated and stored securely via Docker secrets or environment variables.

---

## âœ… License
MIT or internal/private â€” adjust as necessary.
