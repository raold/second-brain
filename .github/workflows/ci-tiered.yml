name: CI/CD Pipeline - Tiered Testing Strategy

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Nightly comprehensive tests
    - cron: '0 2 * * *'

env:
  PYTHONUNBUFFERED: 1
  PYTHONPATH: .
  ENVIRONMENT: test
  TESTING: true

jobs:
  # Stage 1: Smoke Tests (30-60 seconds)
  smoke-tests:
    name: 🔥 Smoke Tests
    runs-on: ubuntu-latest
    timeout-minutes: 3
    outputs:
      should-continue: ${{ steps.smoke-result.outputs.should-continue }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install minimal dependencies
        run: |
          pip install --upgrade pip
          pip install pytest pytest-asyncio httpx psutil
          
      - name: Run smoke tests
        id: smoke-tests
        run: |
          python scripts/ci_runner.py --stage smoke --save-report smoke_report.json
        continue-on-error: false
        
      - name: Determine continuation
        id: smoke-result
        run: |
          if [ ${{ steps.smoke-tests.outcome }} == 'success' ]; then
            echo "should-continue=true" >> $GITHUB_OUTPUT
          else
            echo "should-continue=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Upload smoke test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: smoke-test-results
          path: smoke_report.json

  # Stage 2: Fast Feedback Tests (2-5 minutes)
  fast-feedback:
    name: ⚡ Fast Feedback
    needs: smoke-tests
    if: needs.smoke-tests.outputs.should-continue == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 8
    
    strategy:
      fail-fast: false
      matrix:
        test-group: [unit, integration-basic, api-core]
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r config/requirements-ci.txt
          
      - name: Set up test environment
        run: |
          mkdir -p temp logs
          
      - name: Run fast feedback tests
        run: |
          python scripts/ci_runner.py \
            --stage fast \
            --group ${{ matrix.test-group }} \
            --save-report fast_${{ matrix.test-group }}_report.json
            
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: fast-feedback-results-${{ matrix.test-group }}
          path: fast_${{ matrix.test-group }}_report.json

  # Stage 3: Comprehensive Validation (10-15 minutes)
  comprehensive-validation:
    name: 🔍 Comprehensive Validation
    needs: [smoke-tests, fast-feedback]
    if: |
      needs.smoke-tests.outputs.should-continue == 'true' &&
      (github.ref == 'refs/heads/main' || 
       github.event_name == 'pull_request' ||
       github.event_name == 'schedule')
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_USER: secondbrain
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: secondbrain_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install all dependencies
        run: |
          pip install --upgrade pip
          pip install -r config/requirements-ci.txt
          pip install pytest-cov pytest-xdist pytest-timeout
          
      - name: Set up comprehensive test environment
        env:
          DATABASE_URL: postgresql://secondbrain:test_password@localhost:5432/secondbrain_test
          REDIS_URL: redis://localhost:6379
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-mock' }}
        run: |
          mkdir -p temp logs htmlcov
          
      - name: Run comprehensive tests
        env:
          DATABASE_URL: postgresql://secondbrain:test_password@localhost:5432/secondbrain_test
          REDIS_URL: redis://localhost:6379
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-mock' }}
        run: |
          python scripts/ci_runner.py \
            --stage comprehensive \
            --save-report comprehensive_report.json
            
      - name: Upload comprehensive test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: comprehensive-test-results
          path: |
            comprehensive_report.json
            htmlcov/
            
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: always()
        with:
          file: ./coverage.xml
          flags: comprehensive
          fail_ci_if_error: false

  # Stage 4: Performance Benchmarks (5-20 minutes)
  performance-benchmarks:
    name: 📊 Performance Benchmarks
    needs: [smoke-tests, fast-feedback, comprehensive-validation]
    if: |
      needs.smoke-tests.outputs.should-continue == 'true' &&
      (github.ref == 'refs/heads/main' || github.event_name == 'schedule')
    runs-on: ubuntu-latest
    timeout-minutes: 25
    
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_USER: secondbrain
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: secondbrain_perf
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install performance test dependencies
        run: |
          pip install --upgrade pip
          pip install -r config/requirements-ci.txt
          pip install psutil
          
      - name: Set up performance test environment
        env:
          DATABASE_URL: postgresql://secondbrain:test_password@localhost:5432/secondbrain_perf
          REDIS_URL: redis://localhost:6379
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-mock' }}
          LOG_LEVEL: WARNING
        run: |
          mkdir -p temp logs
          
      - name: Run performance benchmarks
        env:
          DATABASE_URL: postgresql://secondbrain:test_password@localhost:5432/secondbrain_perf
          REDIS_URL: redis://localhost:6379
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-mock' }}
        run: |
          python scripts/ci_runner.py \
            --stage performance \
            --save-report performance_report.json
        continue-on-error: true  # Don't block deployment on performance issues
        
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: |
            performance_report.json
            *.json

  # Final Results Consolidation
  consolidate-results:
    name: 📋 Consolidate Results
    needs: [smoke-tests, fast-feedback, comprehensive-validation, performance-benchmarks]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download all test results
        uses: actions/download-artifact@v3
        with:
          path: test-results/
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install consolidation dependencies
        run: |
          pip install --upgrade pip
          pip install psutil
          
      - name: Consolidate test results
        run: |
          python scripts/consolidate_test_results.py \
            --results-dir test-results/ \
            --output final_test_report.json
            
      - name: Generate test summary
        run: |
          python scripts/generate_test_summary.py \
            --report final_test_report.json \
            --output test_summary.md
            
      - name: Comment test summary on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('test_summary.md')) {
              const summary = fs.readFileSync('test_summary.md', 'utf8');
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            }
            
      - name: Upload final results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: final-test-results
          path: |
            final_test_report.json
            test_summary.md
            
      - name: Determine final status
        id: final-status
        run: |
          python -c "
          import json
          import sys
          
          try:
              with open('final_test_report.json') as f:
                  report = json.load(f)
              
              deployment_ready = report.get('summary', {}).get('deployment_ready', False)
              
              if deployment_ready:
                  print('✅ All critical tests passed - Deployment ready!')
                  sys.exit(0)
              else:
                  print('❌ Critical test failures detected - Deployment blocked')
                  sys.exit(1)
          except Exception as e:
              print(f'❌ Failed to parse test results: {e}')
              sys.exit(1)
          "

  # Notification
  notify:
    name: 📢 Notify Results
    needs: [consolidate-results]
    if: always() && (github.ref == 'refs/heads/main' || failure())
    runs-on: ubuntu-latest
    
    steps:
      - name: Notify on success
        if: needs.consolidate-results.result == 'success'
        run: |
          echo "🎉 CI/CD Pipeline completed successfully!"
          echo "✅ All tests passed - Ready for deployment"
          
      - name: Notify on failure
        if: needs.consolidate-results.result == 'failure'
        run: |
          echo "🚨 CI/CD Pipeline failed!"
          echo "❌ Critical test failures detected"
          echo "Please review test results and fix issues before deployment"

# Workflow-level settings
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Security
permissions:
  contents: read
  pull-requests: write
  checks: write